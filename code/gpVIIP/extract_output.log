
Filename: C:\Users\cmyh\Documents\git\VIGP\code\gpVIIP\mvn_elbo_autolatent_model.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   147    248.4 MiB    248.4 MiB           1       @profile
   148                                             def compute_MV(self):
   149    248.4 MiB      0.0 MiB           1           lsigma2 = self.lsigma2
   150    248.4 MiB      0.0 MiB           1           lLmb = self.lLmb
   151    248.4 MiB      0.0 MiB           1           lnugGPs = self.lnugGPs
   152    248.4 MiB      0.0 MiB           1           ltau2GPs = self.ltau2GPs
   153                                         
   154    248.4 MiB      0.0 MiB           1           if self.clamping:
   155    248.4 MiB      0.0 MiB           1               lLmb, lsigma2, lnugGPs, ltau2GPs = self.parameter_clamp(lLmb, lsigma2, lnugGPs, ltau2GPs)
   156                                         
   157    248.4 MiB      0.0 MiB           1           kap = self.kap
   158    248.4 MiB      0.0 MiB           1           theta = self.theta
   159                                         
   160    248.4 MiB      0.0 MiB           1           n = self.n
   161    248.4 MiB      0.0 MiB           1           G = self.G
   162    248.4 MiB      0.0 MiB           1           sigma2 = torch.exp(lsigma2)
   163                                         
   164    248.4 MiB      0.0 MiB           1           M = torch.zeros(self.kap, self.n)
   165    248.4 MiB      0.0 MiB           1           V = torch.zeros(self.kap, self.n)
   166                                         
   167    296.1 MiB     47.7 MiB           1           Cinvhs = torch.zeros(self.kap, self.n, self.n)
   168   1348.6 MiB      0.0 MiB          26           for k in range(kap):
   169   1340.9 MiB    861.5 MiB          25               C_k = covmat(theta, theta, llmb=lLmb[k], lnug=lnugGPs[k], ltau2=ltau2GPs[k])
   170                                         
   171   1342.9 MiB     47.8 MiB          25               W_k, U_k = torch.linalg.eigh(C_k)
   172                                         
   173   1344.8 MiB     47.8 MiB          25               Ckinvh = U_k / W_k.sqrt()
   174                                         
   175   1348.6 MiB     95.5 MiB          25               Mk = torch.linalg.solve(torch.eye(n) + sigma2 * Ckinvh @ Ckinvh.T, G[k])
   176   1348.6 MiB      0.0 MiB          25               M[k] = Mk
   177   1348.6 MiB      0.0 MiB          25               V[k] = 1 / (1 / sigma2 + (Ckinvh ** 2).sum(1))
   178                                         
   179                                                     # save
   180   1348.6 MiB      0.0 MiB          25               Cinvhs[k] = Ckinvh
   181                                         
   182   1348.6 MiB      0.0 MiB           1           self.M = M
   183   1348.6 MiB      0.0 MiB           1           self.V = V
   184   1300.9 MiB    -47.7 MiB           1           self.Cinvhs = Cinvhs


Filename: C:\Users\cmyh\Documents\git\VIGP\code\gpVIIP\mvn_elbo_autolatent_model.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   107   1299.0 MiB   1299.0 MiB           1       @profile
   108                                             def negelbo(self):
   109   1299.0 MiB      0.0 MiB           1           lLmb = self.lLmb
   110   1299.0 MiB      0.0 MiB           1           lsigma2 = self.lsigma2
   111   1299.0 MiB      0.0 MiB           1           lnugGPs = self.lnugGPs
   112   1299.0 MiB      0.0 MiB           1           ltau2GPs = self.ltau2GPs
   113                                         
   114   1299.0 MiB      0.0 MiB           1           if self.clamping:
   115   1299.0 MiB      0.0 MiB           1               lLmb, lsigma2, lnugGPs, ltau2GPs = self.parameter_clamp(lLmb, lsigma2, lnugGPs, ltau2GPs)
   116                                         
   117   1299.0 MiB      0.0 MiB           1           F = self.F
   118   1299.0 MiB      0.0 MiB           1           theta = self.theta
   119                                         
   120   1299.0 MiB      0.0 MiB           1           m = self.m
   121   1299.0 MiB      0.0 MiB           1           n = self.n
   122   1299.0 MiB      0.0 MiB           1           kap = self.kap
   123                                         
   124   1299.0 MiB      0.0 MiB           1           M = self.M
   125   1299.0 MiB      0.0 MiB           1           V = self.V
   126                                         
   127   1299.0 MiB      0.0 MiB           1           negelbo = 0
   128   2254.1 MiB      0.0 MiB          26           for k in range(kap):
   129   2254.1 MiB    955.1 MiB          25               negloggp_k, Cinvkdiag = negloglik_gp(llmb=lLmb[k], lnug=lnugGPs[k], ltau2=ltau2GPs[k], theta=theta, g=M[k])
   130   2254.1 MiB      0.0 MiB          25               negelbo += negloggp_k
   131   2254.1 MiB      0.0 MiB          25               negelbo += 1 / 2 * (Cinvkdiag * V[k]).sum()
   132                                         
   133   2254.1 MiB      0.0 MiB           1           residF = F - (self.Phi * self.pcw) @ M
   134   2254.1 MiB      0.0 MiB           1           negelbo += m * n / 2 * lsigma2
   135   2254.1 MiB      0.0 MiB           1           negelbo += 1 / (2 * lsigma2.exp()) * (residF ** 2).sum()
   136   2254.1 MiB      0.0 MiB           1           negelbo -= 1 / 2 * torch.log(V).sum()
   137   2254.1 MiB      0.0 MiB           1           negelbo += 1 / (2 * lsigma2.exp()) * V.sum()
   138                                         
   139                                                 # print(negloggp_k.item(), 1 / 2 * (Cinvkdiag * V[k]).sum().item(),
   140                                                 #       m * n / 2 * lsigma2.item(), 1 / (2 * lsigma2.exp().item()) * (residF ** 2).sum().item(),
   141                                                 #       - 1 / 2 * torch.log(V).sum().item(), 1 / (2 * lsigma2.exp().item()) * V.sum().item())
   142                                         
   143   2254.1 MiB      0.0 MiB           1           negelbo += 2 * (lsigma2 - self.lmse0) ** 2
   144                                         
   145   2254.1 MiB      0.0 MiB           1           return negelbo


exit after maximum epoch 6
Filename: C:\Users\cmyh\Documents\git\VIGP\code\gpVIIP\optim_elbo.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
     5    216.9 MiB    216.9 MiB           1   @profile
     6                                         def optim_elbo_lbfgs(model,
     7                                                              maxiter=500, lr=1e-1,
     8                                                              gtol=1e-2,
     9                                                              thetate=None, fte=None,
    10                                                              verbose=False):
    11                                         
    12    217.0 MiB      0.0 MiB           9       optim = torch.optim.FullBatchLBFGS(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)
    13                                         
    14    248.4 MiB -22061.0 MiB          19       def closure():
    15   1299.0 MiB  18926.7 MiB          18           model.compute_MV()
    16   1299.0 MiB      0.0 MiB          18           optim.zero_grad(set_to_none=True)
    17   2254.1 MiB  17202.5 MiB          18           negelbo = model.negelbo()
    18   2254.1 MiB     -0.1 MiB          18           return negelbo
    19    217.0 MiB      0.0 MiB           1       loss_prev = torch.inf
    20   2241.5 MiB      0.0 MiB           1       loss = closure()
    21    237.3 MiB  -2004.2 MiB           1       loss.backward()
    22                                         
    23    237.3 MiB      0.0 MiB           1       epoch = 0
    24                                         
    25    237.3 MiB      0.0 MiB           1       header = ['iter', 'grad.mean()', 'lr', 'negelbo', 'diff.', 'test mse']
    26    237.3 MiB      0.0 MiB           1       if verbose:
    27                                                 print('{:<5s} {:<12s} {:<12s} {:<12s} {:<12s} {:<12s}'.format(*header))
    28                                                 print('{:<5d} {:<12.3f} {:<12.3f} {:<12.3f} {:<12.3f} {:<12.3f}'.format
    29                                                       (epoch, 0, lr, loss, loss_prev - loss, model.test_mse(thetate, fte)))
    30                                             while True:
    31    248.4 MiB      0.0 MiB          12           options = {'closure': closure, 'current_loss': loss,
    32    248.4 MiB      0.0 MiB           6                      'c1': 1e-2, 'c2': 0.7,
    33    248.4 MiB      0.0 MiB           6                      'max_ls': 10, 'damping': True}
    34    248.4 MiB -12032.9 MiB           6           loss, grad, lr, _, _, _, _, _ = optim.step(options)
    35                                         
    36    248.4 MiB      0.0 MiB           6           epoch += 1
    37    248.4 MiB      0.0 MiB           6           if epoch > maxiter:
    38    248.4 MiB      0.0 MiB           1               flag = 'MAX_ITER'
    39    248.4 MiB      0.0 MiB           1               print('exit after maximum epoch {:d}'.format(epoch))
    40    248.4 MiB      0.0 MiB           1               break
    41    248.4 MiB      0.0 MiB           5           if epoch >= 10:
    42                                                     if grad.abs().max() <= gtol:
    43                                                         print('exit after epoch {:d}, GTOL <= {:.3E}'.format(epoch, gtol))
    44                                                         flag = 'G_CONV'
    45                                                         break
    46    248.4 MiB      0.0 MiB           5           if verbose and thetate is not None and fte is not None:
    47                                                     print('{:<5d} {:<12.3f} {:<12.3f} {:<12.3f} {:<12.3f} {:<12.3f}'.format
    48                                                           (epoch, grad.abs().mean(), lr, loss, loss_prev - loss, model.test_mse(thetate, fte)))
    49                                         
    50    248.4 MiB      0.0 MiB           5           with torch.no_grad():
    51    248.4 MiB      0.0 MiB           5               loss_prev = loss.clone()
    52    248.4 MiB      0.0 MiB           1       return model, epoch, flag